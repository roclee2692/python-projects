# -*- coding: utf-8 -*-
"""
B. å°†é€æ—¶æ°”è±¡èšåˆåˆ°â€œæ—¥â€å¹¶ä¸è¡¨å•äºŒåˆå¹¶
è¾“å…¥ï¼š
  - q1_clean.csv            ï¼ˆé€æ—¶æ°”è±¡æ¸…æ´—ç»“æœï¼Œå«åˆ—ï¼štime,T,U,Ff,ff10,VV,Td,RRR,Po,P,Pa,...ï¼‰
  - soil_daily_clean.csv    ï¼ˆè¡¨å•äºŒæ¸…æ´—ç»“æœï¼Œå«åˆ—ï¼šdate, sm5,..., st40, obs48 ç­‰ï¼‰
è¾“å‡ºï¼š
  - met_daily_features.csv  ï¼ˆèšåˆåçš„æ—¥å°ºåº¦æ°”è±¡ç‰¹å¾ï¼‰
  - q1_daily_merged.csv     ï¼ˆä¸åœŸå£¤æ—¥å‡åˆå¹¶åçš„å»ºæ¨¡æ•°æ®ï¼‰
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
import json

# ======== å¯é…ç½®è·¯å¾„ ========
MET_HOURLY_CSV   = r"D:\python-projects\q1_clean.csv"
SOIL_DAILY_CSV   = r"D:\python-projects\soil_daily_clean.csv"
OUT_MET_DAILY    = "met_daily_features.csv"
OUT_MERGED_DAILY = "q1_daily_merged.csv"
REPORT_JSON      = "q1_daily_merge_report.json"

def fail(msg, code=1):
    print("âŒ", msg)
    sys.exit(code)

def ensure_cols(df, need, name):
    miss = [c for c in need if c not in df.columns]
    if miss:
        print(f"âš ï¸  è­¦å‘Šï¼š{name} ç¼ºå°‘åˆ—ï¼š{miss}ï¼ˆå°†å°½é‡ç»§ç»­ï¼‰")
    return miss

def compute_vpd_kpa(T, Td=None, U=None):
    """è¿”å› VPD(kPa)ã€‚ä¼˜å…ˆç”¨ Tdï¼Œæ²¡æœ‰åˆ™ç”¨ U è¿‘ä¼¼ã€‚ç¼ºå¤±åˆ™è¿”å› NaNã€‚"""
    es = 0.6108 * np.exp(17.27*T/(T+237.3))
    if Td is not None:
        ea = 0.6108 * np.exp(17.27*Td/(Td+237.3))
    elif U is not None:
        ea = es * (U/100.0)
    else:
        return np.nan
    return es - ea

def main():
    # 1) è¯»é€æ—¶æ°”è±¡
    p1 = Path(MET_HOURLY_CSV)
    if not p1.exists():
        fail(f"æ‰¾ä¸åˆ°é€æ—¶æ°”è±¡æ–‡ä»¶ï¼š{MET_HOURLY_CSV}")
    met_h = pd.read_csv(MET_HOURLY_CSV)
    if "time" not in met_h.columns:
        fail("é€æ—¶æ°”è±¡ç¼ºå°‘ time åˆ—ï¼Œè¯·ç¡®è®¤ä½ å·²æ‰§è¡Œæ­¥éª¤1ç”Ÿæˆçš„ q1_clean.csvã€‚")

    # è§£ææ—¶é—´ç´¢å¼•
    met_h["time"] = pd.to_datetime(met_h["time"], errors="coerce")
    met_h = met_h.dropna(subset=["time"]).sort_values("time").set_index("time")
    # è‹¥å¸¦æ—¶åŒºï¼Œå»æ‰æ—¶åŒºä»¥ä¾¿ resample
    if met_h.index.tz is not None:
        met_h.index = met_h.index.tz_convert(None)

    # 2) å…³é”®åˆ—å­˜åœ¨æ€§æ£€æŸ¥
    need_any_for_vpd = (("T" in met_h.columns) and (("Td" in met_h.columns) or ("U" in met_h.columns)))
    need_basic = ["T","U","Ff","ff10","VV","RRR","Po","P","Pa"]
    ensure_cols(met_h, need_basic, "é€æ—¶æ°”è±¡")

    # 3) è‹¥æ—  VPD åˆ—åˆ™æŒ‰å°æ—¶å…ˆç®—ä¸€ä¸ª
    if "VPD" not in met_h.columns and need_any_for_vpd:
        T = pd.to_numeric(met_h.get("T"), errors="coerce")
        Td = pd.to_numeric(met_h.get("Td"), errors="coerce") if "Td" in met_h.columns else None
        U  = pd.to_numeric(met_h.get("U"),  errors="coerce") if "U"  in met_h.columns else None
        met_h["VPD"] = compute_vpd_kpa(T, Td=Td, U=U)

    # 4) æ—¥å°ºåº¦èšåˆï¼ˆå‡å€¼/æå€¼/æ€»é‡ï¼‰
    agg_map = {}
    if "T"   in met_h: agg_map["T"]   = "mean"
    if "U"   in met_h: agg_map["U"]   = "mean"
    if "Ff"  in met_h: agg_map["Ff"]  = "mean"
    if "ff10"in met_h: agg_map["ff10"]= "max"
    if "VV"  in met_h: agg_map["VV"]  = "mean"
    if "Td"  in met_h: agg_map["Td"]  = "mean"
    if "VPD" in met_h: agg_map["VPD"] = "mean"
    if "RRR" in met_h: agg_map["RRR"] = "sum"   # æ—¥é™æ°´é‡
    if "Po"  in met_h: agg_map["Po"]  = "mean"
    if "P"   in met_h: agg_map["P"]   = "mean"
    if "Pa"  in met_h: agg_map["Pa"]  = "mean"

    met_d = met_h.resample("D").agg(agg_map)

    # é™„åŠ ï¼šæ—¥æœ€é«˜/æœ€ä½æ°”æ¸©
    if "T" in met_h:
        met_d["T_max"] = met_h["T"].resample("D").max()
        met_d["T_min"] = met_h["T"].resample("D").min()

    met_d = met_d.reset_index().rename(columns={"time":"date"})
    met_d["date"] = pd.to_datetime(met_d["date"]).dt.date
    met_d.to_csv(OUT_MET_DAILY, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²è¾“å‡ºæ—¥å°ºåº¦æ°”è±¡ç‰¹å¾ï¼š{OUT_MET_DAILY}ï¼ˆ{len(met_d)} å¤©ï¼‰")

    # 5) è¯»å–è¡¨å•äºŒï¼ˆæ—¥å‡åœŸå£¤ï¼‰
    p2 = Path(SOIL_DAILY_CSV)
    if not p2.exists():
        fail(f"æ‰¾ä¸åˆ°åœŸå£¤æ—¥å‡æ–‡ä»¶ï¼š{SOIL_DAILY_CSV}ï¼ˆè¯·å…ˆè¿è¡Œè¡¨å•äºŒæ¸…æ´—è„šæœ¬ç”Ÿæˆï¼‰")
    soil = pd.read_csv(SOIL_DAILY_CSV)
    if "date" not in soil.columns:
        # å…¼å®¹ç›´æ¥ä» Excel å¯¼å‡ºçš„æƒ…å†µ
        if "DATE" in soil.columns:
            soil = soil.rename(columns={"DATE":"date"})
        else:
            fail("åœŸå£¤æ—¥å‡æ•°æ®ç¼ºå°‘ date åˆ—ã€‚")
    soil["date"] = pd.to_datetime(soil["date"]).dt.date

    # 6) åˆå¹¶ï¼ˆinnerï¼Œç¡®ä¿ä¸¤è¾¹éƒ½æœ‰ï¼‰
    merged = soil.merge(met_d, on="date", how="inner")
    merged.to_csv(OUT_MERGED_DAILY, index=False, encoding="utf-8-sig")
    print(f"ğŸ‰ å·²è¾“å‡ºåˆå¹¶æ•°æ®ï¼š{OUT_MERGED_DAILY}ï¼ˆ{len(merged)} å¤©ï¼Œå«åœŸå£¤ä¸æ—¥æ°”è±¡ç‰¹å¾ï¼‰")

    # 7) ç®€çŸ­æŠ¥å‘Š
    rpt = {
        "met_daily_rows": int(len(met_d)),
        "soil_daily_rows": int(len(soil)),
        "merged_rows": int(len(merged)),
        "date_range_met": [str(met_d["date"].min()), str(met_d["date"].max())],
        "date_range_soil": [str(soil["date"].min()), str(soil["date"].max())],
        "date_range_merged": [str(merged["date"].min()), str(merged["date"].max())],
        "columns_merged": list(merged.columns)
    }
    Path(REPORT_JSON).write_text(json.dumps(rpt, ensure_ascii=False, indent=2), encoding="utf-8")
    print("ğŸ“„ åˆå¹¶æŠ¥å‘Šï¼š", REPORT_JSON)

    # 8) è´´å¿ƒæç¤ºï¼šé¿å…â€œæœªæ¥ä¿¡æ¯æ³„æ¼â€çš„åšæ³•
    print("\nğŸ”’ æç¤ºï¼šå¦‚æœè¦ç”¨å½“æ—¥æ°”è±¡é¢„æµ‹å½“æ—¥ sm5ï¼Œä¸¥æ ¼åšæ³•æ˜¯ä½¿ç”¨â€œå‰ä¸€æ—¥/å‰å‡ æ—¥â€çš„èšåˆç‰¹å¾ï¼Œ")
    print("   ä¾‹å¦‚å°† met_d å…¨éƒ¨åˆ—åœ¨è®­ç»ƒå‰ shift(1) æˆä¸ºæ˜¨å¤©ç‰¹å¾ï¼Œå†ä¸å½“æ—¥ sm5 åˆå¹¶ã€‚")
    print("   ç¤ºä¾‹ï¼šmerged[[col for col in met_d_cols]] = merged[met_d_cols].shift(1)")

if __name__ == "__main__":
    main()
