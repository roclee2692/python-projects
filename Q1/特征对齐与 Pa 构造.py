# -*- coding: utf-8 -*-
"""
è¯»å–é¢˜å¹²â€œè¡¨2â€ï¼ˆæŸä¸€å¤©é€å°æ—¶æ°”è±¡ï¼‰ï¼Œè‡ªåŠ¨ï¼š
- è¯†åˆ«ç¬¬äºŒè¡Œè¡¨å¤´ï¼ˆæœ‰å¤§æ ‡é¢˜æ—¶ï¼‰
- ç»Ÿä¸€åˆ—åã€æ•°å€¼åŒ–ã€æ—¶é—´(h)->datetime
- è®¡ç®— VPDï¼›DD->è§’åº¦->sin/cosï¼›Pa ç¼ºåˆ™ç”¨ P/Po çš„3å°æ—¶å·®æ„é€ (å‰2å°æ—¶ç½®0å¹¶æ‰“æ ‡)
- åŠ è½½ D:\python-projects\model.pkl ä¸ feature_list.json
- ç‰¹å¾å¯¹é½å¹¶é¢„æµ‹ï¼Œå¯¼å‡º å¸¦é¢„æµ‹ çš„è¡¨æ ¼
è¾“å‡ºç›®å½•ï¼šD:\python-projects\
"""

import re, json, joblib, numpy as np, pandas as pd
from pathlib import Path

# ========= è·¯å¾„é…ç½® =========
IN_TABLE2   = r"C:\Users\Raelon\OneDrive\æ–‡æ¡£\è¡¨2.xlsx"  # é¢˜å¹²ç»™çš„â€œè¡¨2â€æ–‡ä»¶ï¼ˆå¯ä¸º .xlsx æˆ– .csvï¼‰
SHEET_NAME  = 0                                # Excel çš„è¡¨å•åºå·/åç§°
DAY         = "2021-07-15"                     # è¡¨2å¯¹åº”æ—¥æœŸï¼ˆåŠ¡å¿…æ­£ç¡®ï¼‰
OUT_DIR     = Path(r"D:\python-projects")      # ç»Ÿä¸€è¾“å‡ºç›®å½•
MODEL_P     = OUT_DIR / "model.pkl"
FEATS_P     = OUT_DIR / "feature_list.json"

OUT_FEAT_CSV= OUT_DIR / "table2_features.csv"
OUT_PRED_CSV= OUT_DIR / "table2_pred.csv"
OUT_PRED_XLSX= OUT_DIR / "è¡¨2_å¸¦é¢„æµ‹.xlsx"
REPORT_JSON = OUT_DIR / "table2_features_report.json"

# ========= å°å·¥å…· =========
CARDINALS = {"åŒ—":0.0,"ä¸œåŒ—":45.0,"ä¸œ":90.0,"ä¸œå—":135.0,"å—":180.0,"è¥¿å—":225.0,"è¥¿":270.0,"è¥¿åŒ—":315.0}

def read_with_header_fix(path, sheet):
    """é¦–è¡Œè‹¥æ˜¯å¤§æ ‡é¢˜ï¼ˆä»¥#æˆ–â€˜æ°”è±¡ç«™â€™å¼€å¤´ï¼‰ï¼Œåˆ™ç”¨ç¬¬äºŒè¡Œä¸ºè¡¨å¤´(header=1)ã€‚"""
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¡¨2æ–‡ä»¶ï¼š{path}")
    if path.lower().endswith((".xlsx",".xls")):
        probe = pd.read_excel(path, sheet_name=sheet, header=0, nrows=1)
        first_col = str(probe.columns[0])
        header_row = 1 if first_col.startswith("#") or "æ°”è±¡ç«™" in first_col else 0
        df = pd.read_excel(path, sheet_name=sheet, header=header_row)
    else:
        df = pd.read_csv(path)
    return df

def unify_columns(df):
    """ç»Ÿä¸€å¸¸è§åˆ—ååˆ°é¢˜å¹²å­—æ®µ"""
    rename = {
        "æ—¶é—´ï¼ˆhï¼‰":"hour","æ—¶é—´(h)":"hour","æ—¶é—´":"hour","hour":"hour",
        "T":"T","Po":"Po","P":"P","Pa":"Pa","U":"U","DD":"DD","Ff":"Ff",
        "RRR":"RRR","RR":"RRR","R":"RRR","Td":"Td","VV":"VV"
    }
    m = {k:v for k,v in rename.items() if k in df.columns}
    return df.rename(columns=m)

def parse_dd_to_deg(s):
    if pd.isna(s): return np.nan
    s = str(s).strip()
    # å…ˆåŒ¹é…â€œä»â€¦â€¦æ–¹å‘å¹æ¥çš„é£â€
    m = re.search(r"ä»(.+?)æ–¹å‘?å¹æ¥çš„é£", s)
    core = m.group(1) if m else s
    # å»æ‰â€œè½»é£/å¾®é£/é£â€å­—æ ·
    core = re.sub(r"(è½»é£|å¾®é£|å’Œé£|æ¸…åŠ²é£|å¼ºé£|ç–¾é£|é£)", "", core)
    hit = None
    for k in sorted(CARDINALS.keys(), key=len, reverse=True):
        if k in core:
            hit = k; break
    if hit is None:
        # å…œåº•ï¼šæ‰¾ä»»æ„ä¸€ä¸ªæ–¹ä½å­—
        for k in CARDINALS:
            if k in core:
                hit = k; break
    return CARDINALS.get(hit, np.nan)

def compute_vpd_kpa(T, U):
    es = 0.6108 * np.exp(17.27*T/(T+237.3))
    ea = es * (U/100.0)
    return es - ea

# ========= ä¸»æµç¨‹ =========
def main():
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    print("ğŸš€ è¯»å–è¡¨2...")
    raw = read_with_header_fix(IN_TABLE2, SHEET_NAME)
    print(f"âœ… åŸå§‹è¡¨2å½¢çŠ¶ï¼š{raw.shape}")
    print("åŸå§‹åˆ—åé¢„è§ˆï¼š", list(raw.columns)[:12])

    tab = unify_columns(raw)

    # hour -> datetime
    if "hour" not in tab.columns:
        raise ValueError("âŒ è¡¨2ç¼ºå°‘â€˜æ—¶é—´(h)â€™åˆ—ï¼Œè¯·ç¡®è®¤åˆ—åæˆ–æ‰‹åŠ¨æ”¹æˆâ€˜æ—¶é—´(h)â€™/â€˜hourâ€™ã€‚")
    tab["hour"] = tab["hour"].astype(str).str.extract(r"(\d{1,2})").iloc[:,0].astype(int).clip(0,23)
    tab["hour_str"] = tab["hour"].astype(str).str.zfill(2)
    tab["time"] = pd.to_datetime(tab["hour_str"].radd(f"{DAY} ")+":00")

    # æ•°å€¼åŒ–
    for c in ["T","U","Ff","Po","P","Pa","RRR","Td","VV"]:
        if c in tab.columns:
            tab[c] = pd.to_numeric(tab[c], errors="coerce")

    # VPD
    if not {"T","U"}.issubset(tab.columns):
        raise ValueError("âŒ è¡¨2è‡³å°‘éœ€è¦ T ä¸ U æ‰èƒ½è®¡ç®— VPDã€‚")
    tab["VPD"] = compute_vpd_kpa(tab["T"], tab["U"])

    # é£å‘
    if "DD" in tab.columns:
        tab["DD_deg"] = tab["DD"].apply(parse_dd_to_deg)
        tab["sin_dir"] = np.sin(np.radians(tab["DD_deg"]))
        tab["cos_dir"] = np.cos(np.radians(tab["DD_deg"]))
    else:
        tab["DD_deg"] = np.nan
        tab["sin_dir"] = np.nan
        tab["cos_dir"] = np.nan

    # Paï¼šè‹¥ç¼ºåˆ™ç”¨ P æˆ– Po çš„3å°æ—¶å·®æ„é€ ï¼ˆå‰2å°æ—¶ç½®0ï¼‰
    if ("Pa" not in tab.columns) or tab["Pa"].isna().all():
        src = "P" if "P" in tab.columns else ("Po" if "Po" in tab.columns else None)
        if src:
            tab = tab.sort_values("time")
            tab["Pa"] = tab[src] - tab[src].shift(3)
            tab.loc[tab["hour"] < 3, "Pa"] = 0.0
            tab["has_Pa"] = 0
            print(f"â„¹ï¸ æœªæä¾› Paï¼Œç”¨ {src} æ„é€  3 å°æ—¶å·®ï¼›å‰ 2 å°æ—¶ç½® 0ã€‚")
        else:
            tab["Pa"] = 0.0
            tab["has_Pa"] = 0
            print("â„¹ï¸ æ—  P/Po å¯æ„é€  Paï¼Œç»Ÿä¸€ç½® 0ã€‚")
    else:
        tab["has_Pa"] = tab["Pa"].notna().astype(int)
        tab["Pa"] = tab["Pa"].fillna(0.0)

    # RRR ç¼ºå¤±ç½®0
    if "RRR" not in tab.columns:
        tab["RRR"] = 0.0
    else:
        tab["RRR"] = tab["RRR"].fillna(0.0)

    # å¯¼å‡ºç‰¹å¾ä¸­é—´è¡¨
    used_cols = ["time","hour","T","U","Ff","Po","P","Pa","has_Pa","RRR","DD","DD_deg","sin_dir","cos_dir","VPD","Td","VV"]
    used_cols = [c for c in used_cols if c in tab.columns]
    feat = tab[used_cols].copy()
    feat.to_csv(OUT_FEAT_CSV, index=False, encoding="utf-8-sig")

    # è¯»å–æ¨¡å‹ä¸ç‰¹å¾æ¸…å•
    model = joblib.load(MODEL_P)
    feats = json.loads(Path(FEATS_P).read_text(encoding="utf-8"))["feats"]

    # å°†â€œå½“å‰å°æ—¶â€çš„åŸºç¡€åˆ—æ˜ å°„åˆ°è®­ç»ƒç”¨çš„ *_lag1 åˆ—ï¼›å…¶ä½™ï¼ˆç´¯è®¡/å·®åˆ†ï¼‰ç”¨ 0 å…œåº•
    X = pd.DataFrame(index=feat.index)
    base_map = {
        "T":"T_lag1","U":"U_lag1","Ff":"Ff_lag1","VV":"VV_lag1","Td":"Td_lag1","VPD":"VPD_lag1",
        "RRR":"RRR_lag1","Po":"Po_lag1","P":"P_lag1","Pa":"Pa_lag1","T_max":"T_max_lag1","T_min":"T_min_lag1"
    }
    for base, lagname in base_map.items():
        if (lagname in feats) and (base in feat.columns):
            X[lagname] = feat[base]
    # å…¶ä½™è®­ç»ƒç‰¹å¾è¡¥ 0
    missing_for_pred = []
    for col in feats:
        if col not in X.columns:
            X[col] = 0.0
            missing_for_pred.append(col)
    X = X[feats]

    # é¢„æµ‹
    pred = model.predict(X)

    # å›å†™åˆ°åŸè¡¨ï¼ˆä¿ç•™åŸæ ·+æ–°å¢åˆ—ï¼‰
    out = raw.copy()
    out["predicted_5cm_SM"] = pred
    out.to_csv(OUT_PRED_CSV, index=False, encoding="utf-8-sig")
    try:
        with pd.ExcelWriter(OUT_PRED_XLSX, engine="openpyxl") as w:
            out.to_excel(w, index=False, sheet_name="è¡¨2_å¸¦é¢„æµ‹")
    except Exception:
        pass

    # æŠ¥å‘Š
    report = {
        "rows": int(len(feat)),
        "dd_mapped": int(feat["sin_dir"].notna().sum()),
        "pa_nonzero": int((feat["Pa"].abs() > 1e-9).sum()),
        "vpd_min": float(np.nanmin(feat["VPD"])) if "VPD" in feat.columns else None,
        "vpd_mean": float(np.nanmean(feat["VPD"])) if "VPD" in feat.columns else None,
        "vpd_max": float(np.nanmax(feat["VPD"])) if "VPD" in feat.columns else None,
        "missing_features_filled0": missing_for_pred
    }
    REPORT_JSON.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")

    # æ§åˆ¶å°æ‘˜è¦
    print("\nğŸ‰ å¤„ç†ä¸é¢„æµ‹å®Œæˆ")
    print(f"- ç‰¹å¾ä¸­é—´è¡¨ï¼š{OUT_FEAT_CSV}")
    print(f"- å¸¦é¢„æµ‹CSVï¼š{OUT_PRED_CSV}")
    print(f"- å¸¦é¢„æµ‹Excelï¼š{OUT_PRED_XLSX}")
    print(f"- æŠ¥å‘Šï¼š{REPORT_JSON}")
    print(f"- é£å‘æˆåŠŸæ˜ å°„ï¼š{report['dd_mapped']} / {report['rows']}")
    print(f"- Pa éé›¶å°æ—¶æ•°ï¼š{report['pa_nonzero']}")
    print(f"- VPDèŒƒå›´ï¼šmin={report['vpd_min']:.3f} mean={report['vpd_mean']:.3f} max={report['vpd_max']:.3f}")
    if report["missing_features_filled0"]:
        print("âš ï¸ ä»¥ä¸‹è®­ç»ƒç‰¹å¾åœ¨è¡¨2ä¸­æ— æ³•è·å¾—ï¼Œå·²ç½®0ï¼š")
        print(report["missing_features_filled0"][:20], "â€¦ï¼ˆå¦‚æœ‰æ›´å¤šç•¥ï¼‰")

if __name__ == "__main__":
    main()
